â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                       ROS2 Workspace                       â”‚
â”‚                            (colcon)                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  perception/                                               â”‚
â”‚     â”œâ”€â”€ camera_driver_node         (RealSense/ZED ë“±)      â”‚
â”‚     â”œâ”€â”€ depth_preprocess_node      (OpenCV)                â”‚
â”‚     â”œâ”€â”€ yolo_detection_node        (YOLOv8/YOLO-NAS)       â”‚
â”‚     â”œâ”€â”€ hand_tracking_node         (HRI ëª¨ë“ˆ)              â”‚
â”‚     â””â”€â”€ object_tracking_node       (OpenCV KCF/CSRT)       â”‚
â”‚                                                             
â”‚  vlm_api/                                                   â”‚
â”‚     â””â”€â”€ vlm_query_node             (Vision-Language Model) â”‚
â”‚                                                             
â”‚  manipulation/                                              â”‚
â”‚     â”œâ”€â”€ motion_planner_node        (MoveIt2)               â”‚
â”‚     â”œâ”€â”€ grasp_planner_node         (grasp pose)            â”‚
â”‚     â””â”€â”€ pick_place_node            (Python main)           â”‚
â”‚                                                             
â”‚  robot/                                                      â”‚
â”‚     â”œâ”€â”€ doosan_driver              (DSR ROS2)              â”‚
â”‚     â”œâ”€â”€ tf_broadcaster             (camera â†” URDF)         â”‚
â”‚     â””â”€â”€ bringup.launch.py                                    â”‚
â”‚                                                             
â”‚  hri/                                                        â”‚
â”‚     â””â”€â”€ gesture_interface_node     (ì†ì¶”ì  ì´ë²¤íŠ¸)         â”‚
â”‚                                                             
â”‚  docker/                                                     â”‚
â”‚     â”œâ”€â”€ Dockerfile.cpu                                       â”‚
â”‚     â””â”€â”€ docker-compose.yaml                                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜


(1) Wrist Depth Camera â†’ YOLO + VLM íŒŒì´í”„ë¼ì¸
Depth Camera â†’ depth_preprocess â†’ YOLO Detection â†’  
      â”œâ”€ object_tracking_node (OpenCV) â†’ Pick&Place  
      â””â”€ hand_tracking_node (HRI) â†’ gesture_interface

(2) Pick & Place íŒŒì´í”„ë¼ì¸
object_tracking_node â†’ grasp_planner â†’ motion_planner â†’ doosan_driver

(3) HRI ì† ì œìŠ¤ì²˜ íŒŒì´í”„ë¼ì¸
hand_tracking_node (YOLO + keypoints) â†’ gesture_interface â†’ pick_place_node


ì˜ˆ:

ì†ë°”ë‹¥ ì—´ê¸° â†’ â€œì •ì§€â€

ì§‘ê²Œì†(í•€ì¹­) â†’ â€œì§‘ê¸° ì‹œì‘â€

ì—„ì§€ ì˜¬ë¦¬ê¸° â†’ â€œë‹¤ìŒ ì‘ì—… ìš”ì²­â€


# ì‚°ì—…ìš© 

ros2_ws/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ dsr_description/        # Doosan URDF + SRDF + meshes
â”‚   â”œâ”€â”€ dsr_moveit_config/      # MoveIt2 config
â”‚   â”œâ”€â”€ perception_pkg/         # YOLO, Tracker, DepthFusion, Kalman
â”‚   â”‚    â”œâ”€â”€ launch/
â”‚   â”‚    â”‚    â””â”€â”€ perception.launch.py
â”‚   â”‚    â”œâ”€â”€ config/
â”‚   â”‚    â”‚    â””â”€â”€ camera.yaml
â”‚   â”‚    â””â”€â”€ src/
â”‚   â”‚         â”œâ”€â”€ perception_node.py
â”‚   â”‚         â”œâ”€â”€ hand_detector.py
â”‚   â”‚         â””â”€â”€ pointcloud_node.py
â”‚   â”œâ”€â”€ calibration_pkg/
â”‚   â”‚    â””â”€â”€ wrist_cam_calibration.py
â”‚   â”œâ”€â”€ visual_servoing_pkg/
â”‚   â”‚    â”œâ”€â”€ launch/
â”‚   â”‚    â””â”€â”€ src/
â”‚   â”‚         â”œâ”€â”€ vs_control_node.py
â”‚   â”‚         â””â”€â”€ vs_controller.py
â”‚   â”œâ”€â”€ pickplace_pkg/
â”‚   â”‚    â”œâ”€â”€ action/
â”‚   â”‚    â”‚    â””â”€â”€ PickPlace.action
â”‚   â”‚    â”œâ”€â”€ launch/
â”‚   â”‚    â””â”€â”€ src/
â”‚   â”‚         â”œâ”€â”€ pickplace_server.py
â”‚   â”‚         â””â”€â”€ pickplace_client.py
â”‚   â”œâ”€â”€ hri_pkg/
â”‚   â”‚    â”œâ”€â”€ launch/
â”‚   â”‚    â””â”€â”€ src/
â”‚   â”‚         â”œâ”€â”€ gesture_node.py
â”‚   â”‚         â”œâ”€â”€ safety_monitor.py
â”‚   â”‚         â””â”€â”€ speech_cmd_node.py   # ìŒì„± ëª…ë ¹ â†’ ë™ì‘ ë§¤í•‘
â”‚   â”œâ”€â”€ rviz_config/
â”‚   â”‚    â”œâ”€â”€ rviz2_cam_pc.rviz
â”‚   â”‚    â””â”€â”€ dsr_moveit.rviz
â”‚   â”œâ”€â”€ docker/
â”‚   â”‚    â”œâ”€â”€ Dockerfile
â”‚   â”‚    â””â”€â”€ docker-compose.yml
â”‚   â”œâ”€â”€ bringup_pkg/
â”‚   â”‚    â”œâ”€â”€ launch/
â”‚   â”‚    â”‚    â””â”€â”€ full_bringup.launch.py
â”‚   â”‚    â””â”€â”€ src/
â”‚   â”‚         â””â”€â”€ bringup_node.py
â””â”€â”€ colcon.meta


==================================================================================================================

#!/bin/bash
mkdir -p ros2_ws/src
cd ros2_ws/src

# packages
pkgs=(
  perception_pkg
  calibration_pkg
  visual_servoing_pkg
  pickplace_pkg
  hri_pkg
  bringup_pkg
)

for p in "${pkgs[@]}"; do
  ros2 pkg create $p --build-type ament_python
done

# moveit & description íŒ¨í‚¤ì§€ ìë™ ë³µì‚¬
git clone https://github.com/doosan-robotics/doosan-robot2.git dsr_description2
git clone https://github.com/doosan-robotics/doosan-robot2-moveit.git dsr_moveit_config

cd ..
colcon build


#
# í–¥í›„ ê³„íš 

ì‹ ë¢°ì„± 100%ìš© ì»¨í…Œì´ë„ˆ ìµœì í™”(Docker + GPU + ROS2)

Doosan ë¡œë´‡ ì‹¤ì‚¬ìš© ì†ë„ ê¸°ë°˜ ì¶©ëŒ íšŒí”¼ ëª¨ë¸ë§

AI ê¸°ë°˜ ê·¸ë¦½ í¬ì¸íŠ¸ ìë™ ìµœì í™” (6D pose + grasp detection)

ì‚°ì—…ìš© ì•ˆì „ ê·œê²©(ISO 10218, TS 15066) ì¤€ìˆ˜ êµ¬ì¡° ì„¤ê³„

#
#
# ì „ì²´ ì‹œìŠ¤í…œ ì‹¤í–‰ ëª…ë ¹
docker-compose up --build


ë˜ëŠ” ë¡œì»¬ì—ì„œ:

ros2 launch bringup_pkg full_bringup.launch.py


4) Boston Dynamics ìŠ¤íƒ€ì¼ í–‰ë™ FSM / BehaviorTree

ì‚°ì—…ìš© ë¡œë´‡ì—ì„œ ê°€ì¥ ì•ˆì •ì ì¸ êµ¬ì¡°ëŠ”:

HRI â†’ Task FSM â†’ BehaviorTree â†’ Skills(Pick/Place/Move/VS/Safety)

âœ” BehaviorTree êµ¬ì¡°
í–‰ë™ íŠ¸ë¦¬ ì˜ˆì‹œ
Root
 â””â”€ Sequence
      â”œâ”€ IsHumanSafe?
      â”œâ”€ DetectObject
      â”œâ”€ GenerateGraspPose
      â”œâ”€ VisualServoApproach
      â”œâ”€ ExecutePick
      â”œâ”€ MoveToPlaceLocation
      â””â”€ ExecutePlace

âœ” ROS2 BehaviorTree.CPP ë…¸ë“œ (C++)
bt_pkg/nodes/check_human_safe.cpp
class IsHumanSafe : public BT::ConditionNode {
public:
    IsHumanSafe(const std::string& name)
        : BT::ConditionNode(name, {}) {
        sub_ = node.create_subscription<std_msgs::msg::Bool>(
            "/safety/human_safe", 10,
            [this](auto msg){ safe_ = msg->data; });
    }

    BT::NodeStatus tick() override {
        return safe_ ? BT::NodeStatus::SUCCESS
                     : BT::NodeStatus::FAILURE;
    }

private:
    bool safe_ = true;
    rclcpp::Node node{"safe_checker"};
    rclcpp::Subscription<std_msgs::msg::Bool>::SharedPtr sub_;
};

âœ” ê³ ìˆ˜ì¤€ FSM (Python)
fsm_pkg/task_fsm.py
from transitions import Machine

class TaskFSM(object):
    states = ["IDLE", "DETECT", "GRASP", "PICK", "PLACE", "ERROR"]

    def __init__(self):
        self.machine = Machine(model=self, states=TaskFSM.states, initial="IDLE")

        self.machine.add_transition("start", "IDLE", "DETECT")
        self.machine.add_transition("object_found", "DETECT", "GRASP")
        self.machine.add_transition("grasp_ready", "GRASP", "PICK")
        self.machine.add_transition("picked", "PICK", "PLACE")
        self.machine.add_transition("placed", "PLACE", "IDLE")

        self.machine.add_transition("fault", "*", "ERROR")



        â‘  2~3ëŒ€ ì¹´ë©”ë¼ Multi-View â†’ Birdâ€™s Eye Workspace Map
(â€œTop-Down Workspace Understanding for Manipulation + Safetyâ€)
âœ” ì „ì²´ êµ¬ì¡° (í‚¤ í¬ì¸íŠ¸)
Camera1 â”€â”€â”€â”€â”€â”€\
Camera2 â”€â”€â”€â”€â”€â”€â”€â”€â†’ MultiCam Calibration â†’ Unified Extrinsic (T_camâ†’world)
Camera3 â”€â”€â”€â”€â”€â”€/   
                             â†“
  RGB+D â†’ YOLO â†’ Person/Object Detector
                             â†“
  Multi-view 3D Fusion (Triangulation, TSDF-VoxFusion)
                             â†“
  Birdâ€™s-Eye 2D Map or 3D Occupancy (Octomap/TSDF)
                             â†“
  Robot Safety + PickPlace Planning + HRI Zone Control

âœ” ROS2 íŒ¨í‚¤ì§€ êµ¬ì„±
multicam_pkg/
 â”œâ”€ launch/
 â”‚   â””â”€ multicam_bird_view.launch.py
 â”œâ”€ src/
 â”‚   â”œâ”€ multicam_sync.py          (3ì¹´ë©”ë¼ ë™ê¸°í™”)
 â”‚   â”œâ”€ multicam_extrinsic_node.py (Extrinsics ìë™ ë³´ì •)
 â”‚   â”œâ”€ multicam_fusion_node.py    (3D fusion + bird-eye map)
 â”‚   â””â”€ human_zoning_node.py       (YOLO 3D bounding box + Zone)
 â””â”€ config/
     â”œâ”€ cam1.yaml
     â”œâ”€ cam2.yaml
     â”œâ”€ cam3.yaml
     â””â”€ world.yaml

ğŸŸ¦ A. Multi-Camera Extrinsic ìë™ ë³´ì •

ì¹´ë©”ë¼ 2â€“3ëŒ€ê°€ ê°ê°:

Cam1 â†’ world
Cam2 â†’ world
Cam3 â†’ world


ì´ extrinsic TFë¥¼ ìë™ìœ¼ë¡œ ì‚°ì¶œí•´ì•¼ í•¨.

ë°©ë²•:

âœ” ArUco Marker ìë™ solvePnP
âœ” ë˜ëŠ” TSDF Reconstruction + ICP Matching

(ì¹´ë©”ë¼ë¼ë¦¬ ë³´ì§€ ì•Šì•„ë„ ë˜ëŠ” ë°©ë²•)

ğŸ“Œ í•µì‹¬ ì½”ë“œ: multicam_extrinsic_node.py
T = cv2.solvePnP(objPoints, imgPoints)
R, _ = cv2.Rodrigues(T[1])
t = T[2]

T_cam_world = np.eye(4)
T_cam_world[:3,:3] = R
T_cam_world[:3,3] = t.ravel()

# TF broadcast
br.sendTransform(tf2_ros.TransformStamped from T_cam_world)

ğŸŸ¦ B. Multi-View â†’ Birdâ€™s-Eye Map

2~3 ì¹´ë©”ë¼ depth mapì„ TSDFë¡œ fuse.

í•µì‹¬:
TSDF Fusion(3 cam) â†’ Voxel â†’ Project â†’ Top-Down Map (BEV)

ğŸ“Œ multicam_fusion_node.py
vol = o3d.pipelines.integration.ScalableTSDFVolume(
    voxel_length=0.005,
    sdf_trunc=0.03,
    color_type=o3d.pipelines.integration.TSDFVolumeColorType.RGB8)

for cam in [1,2,3]:
    rgbd = create_rgbd(cam)
    extr = T_world_cam[cam]
    vol.integrate(rgbd, intrinsics[cam], extr.inverse())

mesh = vol.extract_triangle_mesh()
bev_map = compute_bev(mesh)

ğŸŸ¦ C. Birdâ€™s-Eye Human Zoning (YOLO + 3D Bounding Box)
ì…ë ¥:

YOLO bounding box (u, v)

Multi-camera â†’ triangulated 3D person centroid

Depth = from nearest camera

3D ìœ„ì¹˜ ì¶”ì •:
Ray1 âˆ© Ray2 âˆ© Ray3 = Person 3D center

Zone ì •ì˜:
Zone A (0â€“1m): Emergency Stop
Zone B (1â€“2m): Speed Limit 25%
Zone C (2m+): Normal

ğŸ“Œ human_zoning_node.py
person_3d = triangulate(cam1_bbox, cam2_bbox, cam3_bbox)

distance = norm(person_3d - robot_base)

if distance < 1.0:
    pub_zone("STOP")
elif distance < 2.0:
    pub_zone("SLOW")
else:
    pub_zone("NORMAL")

ğŸŸ¥ â‘¡ Visual Servoing (Eye-in-Hand) + 6D GraspNet Fusion
âœ” ì „ì²´ Pipeline
Wrist Camera (eye-in-hand)
      â†“
YOLO segmentation (object)
      â†“
GraspNet 6D grasp candidates
      â†“
Visual Servoing Controller (IBVS or PBVS)
      â†“
MoveIt2 Cartesian Servo (cartesian velocity command)
      â†“
Pick â†’ Lift â†’ Place

âœ” Visual Servoing(Eye-in-Hand) ê³µì‹

Image-Based VS (IBVS)

v = -Î» * Lâº * e


Where:

e = feature error (centroid, contour, keypoints)

Lâº = pseudo inverse of interaction matrix

v = Cartesian 6-DoF velocity command to robot

âœ” 6D GraspNetê³¼ Visual Servoing ì—°ê²° ë°©ì‹

GraspNetì´ â€œì´ë¡ ì ì¸â€ 6D grasp pose ì œê³µ

VSê°€ ê·¸ grasp poseë¡œ ë¡œë´‡ end-effectorë¥¼ ë³´ì • ì´ë™
(ë¯¸ì„¸ ì¡°ì • / sub-millimeter alignment)

ì¦‰:

GraspNet â†’ ëª©í‘œ ìì„¸
VS â†’ fine alignment ìˆ˜í–‰

ğŸŸ¦ VS Controller ROS2 Node
ğŸ“Œ vs_control_node.py
import numpy as np
from geometry_msgs.msg import Twist

class VSController(Node):
    def __init__(self):
        self.sub_err = self.create_subscription(
            PixelError, "/vs/error", self.cb_err, 10)
        self.pub_cmd = self.create_publisher(
            Twist, "/servo_server/cmd_vel", 10)

        self.lambda_gain = 0.8

    def cb_err(self, msg):
        e = np.array([msg.ex, msg.ey, msg.ez, msg.erx, msg.ery, msg.erz])
        L = compute_interaction_matrix(msg)
        v = -self.lambda_gain * np.linalg.pinv(L).dot(e)

        cmd = Twist()
        cmd.linear.x  = v[0]
        cmd.linear.y  = v[1]
        cmd.linear.z  = v[2]
        cmd.angular.x = v[3]
        cmd.angular.y = v[4]
        cmd.angular.z = v[5]
        self.pub_cmd.publish(cmd)

ğŸŸ¦ 6D GraspNet + VS â€œTask Policyâ€
Sequence:

Multi-camera BEV â†’ pickable object í™•ì¸

Wrist-cam YOLO â†’ object local seg

GraspNet â†’ 6D grasp candidate

VS Align â†’ ì ‘ì´‰ ì „ ë¯¸ì„¸ ì •ë ¬

Grasp â†’ Lift â†’ Place

ğŸŸ© ì „ì²´ í†µí•© Launch (ì™„ì„±íŒ)
multicam_bird_vs_grasp.launch.py
[Camera1]
[Camera2]
[Camera3]
   â†’ Extrinsic Calibration
   â†’ Multi-View Fusion
   â†’ BEV Map + Zoning (Human Safety)
[WristCam]
   â†’ YOLO
   â†’ GraspNet
   â†’ Visual Servoing
[MoveIt2]
   â†’ Grasp Execution
